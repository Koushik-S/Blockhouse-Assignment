{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "market_features_df=pd.read_csv(\"/content/market_features_df_new.csv\")\n",
        "state_columns = ['Close', 'Volume', 'RSI', 'MACD', 'MACD_signal', 'Stoch_k', 'Stoch_d',\n",
        "                 'OBV', 'Upper_BB', 'Middle_BB', 'Lower_BB', 'ATR_1', 'ADX', '+DI', '-DI', 'CCI']\n",
        "#market_features_df[state_columns] = scaler.fit_transform(market_features_df[state_columns])"
      ],
      "metadata": {
        "id": "jKgBpZCX4Nxa"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Transformer structure"
      ],
      "metadata": {
        "id": "66PeULX539Nj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, model_dim, num_heads, num_layers, output_dim):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Linear(input_dim, model_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(model_dim, output_dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.embedding(src)\n",
        "        src = src.permute(1, 0, 2)  # Transformer expects (seq_len, batch_size, feature_dim)\n",
        "        transformer_output = self.transformer_encoder(src)\n",
        "        output = self.fc(transformer_output.mean(dim=0))\n",
        "        return output\n",
        "\n",
        "# Model parameters\n",
        "input_dim = len(state_columns)\n",
        "model_dim = 64\n",
        "num_heads = 4\n",
        "num_layers = 2\n",
        "output_dim = 3"
      ],
      "metadata": {
        "id": "zI61Wl-B4tNV"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Trading Environment for transformer"
      ],
      "metadata": {
        "id": "wHwaDBcW347b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class TradingEnvironment(gym.Env):\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, data, daily_trading_limit, transformer_model_path, seq_length=50):\n",
        "        super(TradingEnvironment, self).__init__()\n",
        "        self.data = data.reset_index(drop=True)  # Ensure the data index is reset\n",
        "        self.daily_trading_limit = daily_trading_limit\n",
        "        self.seq_length = seq_length\n",
        "        self.current_step = 0\n",
        "\n",
        "        # Extract state columns\n",
        "        self.state_columns = ['Close', 'Volume', 'RSI', 'MACD', 'MACD_signal', 'Stoch_k', 'Stoch_d',\n",
        "                              'OBV', 'Upper_BB', 'Middle_BB', 'Lower_BB', 'ATR_1', 'ADX', '+DI', '-DI', 'CCI']\n",
        "\n",
        "        # Fit the scaler on the training data\n",
        "        self.scaler = StandardScaler()\n",
        "        self.scaler.fit(self.data[self.state_columns])\n",
        "\n",
        "        # Load the transformer model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = TransformerModel(len(self.state_columns), 64, 4, 2, 3).to(self.device)\n",
        "        self.model.load_state_dict(torch.load(transformer_model_path, map_location=self.device))\n",
        "        self.model.eval()\n",
        "\n",
        "        # Initialize balance, shares held, and total shares traded\n",
        "        self.balance = 10_000_000.0  # $10 million\n",
        "        self.shares_held = 0\n",
        "        self.total_shares_traded = 0\n",
        "\n",
        "        # Define action space: [Hold, Buy, Sell]\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "\n",
        "        # Define observation space based on state columns\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf, high=np.inf, shape=(len(self.state_columns),), dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        self.balance = 10_000_000.0  # $10 million\n",
        "        self.shares_held = 0\n",
        "        self.total_shares_traded = 0\n",
        "        self.cumulative_reward = 0\n",
        "        self.trades = []\n",
        "        return self._next_observation()\n",
        "\n",
        "    def _next_observation(self):\n",
        "        return self.data[self.state_columns].iloc[self.current_step].values\n",
        "\n",
        "    def _get_scaled_observation(self):\n",
        "        obs_df = self.data[self.state_columns].iloc[[self.current_step]]\n",
        "        scaled_obs = self.scaler.transform(obs_df).flatten()\n",
        "        return scaled_obs\n",
        "\n",
        "    def step(self, action):\n",
        "        expected_price = self.data.iloc[self.current_step]['ask_px_00']\n",
        "        actual_price = self.data.iloc[self.current_step]['price']\n",
        "        transaction_time = self.data.iloc[self.current_step]['ts_in_delta']\n",
        "        self._take_action(action)\n",
        "        reward = 0\n",
        "\n",
        "        if self.current_step >= len(self.data) - 1:\n",
        "            done = True\n",
        "        else:\n",
        "            done = False\n",
        "        if action != 0:\n",
        "            transaction_cost = self._calculate_transaction_cost(\n",
        "                self.data.iloc[self.current_step]['Volume'], 0.3, self.data['Volume'].mean()\n",
        "            )\n",
        "            reward = self._calculate_reward(expected_price, actual_price, transaction_time, transaction_cost)\n",
        "            self.cumulative_reward += reward\n",
        "            if self.trades:\n",
        "                self.trades[-1]['reward'] = reward\n",
        "                self.trades[-1]['transaction_cost'] = transaction_cost\n",
        "                self.trades[-1]['slippage'] = expected_price - actual_price\n",
        "                self.trades[-1]['time_penalty'] = 100 * transaction_time / 1e9\n",
        "\n",
        "        info = {\n",
        "            'step': self.current_step,\n",
        "            'action': action,\n",
        "            'price': actual_price,\n",
        "            'shares': self.trades[-1]['shares'] if self.trades else 0\n",
        "        }\n",
        "        self.current_step += 1\n",
        "\n",
        "        return self._next_observation(), reward, done, info\n",
        "\n",
        "    def _take_action(self, action):\n",
        "        current_price = self.data.iloc[self.current_step]['Close']\n",
        "        current_time = pd.to_datetime(self.data.iloc[self.current_step]['ts_event'])\n",
        "        trade_info = {'step': self.current_step, 'timestamp': current_time, 'action': action, 'price': current_price, 'shares': 0, 'reward': 0, 'transaction_cost': 0, 'slippage': 0, 'time_penalty': 0}\n",
        "\n",
        "        if action == 1:  # Buy\n",
        "            shares_bought = (self.balance * np.random.uniform(0.001, 0.005)) // current_price\n",
        "            self.balance -= shares_bought * current_price\n",
        "            self.shares_held += shares_bought\n",
        "            self.total_shares_traded += shares_bought\n",
        "            trade_info['shares'] = shares_bought\n",
        "            if shares_bought > 0:\n",
        "                self.trades.append(trade_info)\n",
        "        elif action == 2:  # Sell\n",
        "            shares_sold = min((self.balance * np.random.uniform(0.001, 0.005)) // current_price, self.shares_held)\n",
        "            self.balance += shares_sold * current_price\n",
        "            self.shares_held -= shares_sold\n",
        "            self.total_shares_traded -= shares_sold\n",
        "            trade_info['shares'] = shares_sold\n",
        "            if shares_sold > 0:\n",
        "                self.trades.append(trade_info)\n",
        "\n",
        "    def _calculate_reward(self, expected_price, actual_price, transaction_time, transaction_cost):\n",
        "        slippage = expected_price - actual_price\n",
        "        time_penalty = 100 * transaction_time / 1e9\n",
        "        reward = - (slippage + time_penalty + transaction_cost)\n",
        "        return reward\n",
        "\n",
        "    def _calculate_transaction_cost(self, volume, volatility, daily_volume):\n",
        "        return volatility * np.sqrt(volume / daily_volume)\n",
        "\n",
        "    def run(self):\n",
        "        self.reset()\n",
        "        for _ in range(len(self.data) - 1):\n",
        "            scaled_obs = self._get_scaled_observation()\n",
        "            with torch.no_grad():\n",
        "                obs_tensor = torch.tensor(scaled_obs, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(self.device)  # Add batch and sequence dimension\n",
        "                action_probs = self.model(obs_tensor)\n",
        "                action = torch.argmax(action_probs, dim=1).item()\n",
        "            obs, reward, done, info = self.step(action)\n",
        "            print(f\"Step: {info['step']}, Action: {info['action']}, Price: {info['price']}, Shares: {info['shares']}, Reward: {reward}\")\n",
        "            if done:\n",
        "                break\n",
        "        return self.cumulative_reward, self.trades\n",
        "\n",
        "    def render(self, mode='human', close=False):\n",
        "        print(f'Step: {self.current_step}')\n",
        "        print(f'Balance: {self.balance}')\n",
        "        print(f'Shares held: {self.shares_held}')\n",
        "        print(f'Total shares traded: {self.total_shares_traded}')\n",
        "        print(f'Total portfolio value: {self.balance + self.shares_held * self.data.iloc[self.current_step][\"Close\"]}')\n",
        "        print(f'Cumulative reward: {self.cumulative_reward}')\n",
        "        self.print_trades()\n",
        "\n",
        "    def print_trades(self):\n",
        "        trades_df = pd.DataFrame(self.trades)\n",
        "        trades_df.to_csv('trades_transformer.csv', index=False)\n",
        "        #for trade in self.trades:\n",
        "        #    print(f\"Step: {trade['step']}, Timestamp: {trade['timestamp']}, Action: {trade['action']}, Price: {trade['price']}, Shares: {trade['shares']}, Reward: {trade['reward']}, Transaction Cost: {trade['transaction_cost']}, Slippage: {trade['slippage']}, Time Penalty: {trade['time_penalty']}\")\n"
      ],
      "metadata": {
        "id": "5QaKWKsm3syE"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Implement the environment for trade"
      ],
      "metadata": {
        "id": "0DslqQG24I3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the daily trading limit (total number of shares to trade per day)\n",
        "daily_trading_limit = 1000\n",
        "market_features_df=pd.read_csv(\"/content/market_features_df_new.csv\")\n",
        "ticker = 'AAPL'  # Specify the ticker you want to trade\n",
        "ticker_data = market_features_df[market_features_df['symbol'] == ticker]\n",
        "\n",
        "# Create the trading environment\n",
        "transformer_model_path = 'transformer_model_v1.pth'\n",
        "env = TradingEnvironment(ticker_data, daily_trading_limit, transformer_model_path, seq_length=50)\n",
        "\n",
        "# Evaluate the transformer model in the environment\n",
        "obs = env.reset()\n",
        "for _ in range(len(ticker_data) - 1):\n",
        "    scaled_obs = env._get_scaled_observation()\n",
        "    with torch.no_grad():\n",
        "        obs_tensor = torch.tensor(scaled_obs, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(env.device)  # Add batch and sequence dimension\n",
        "        action_probs = env.model(obs_tensor)\n",
        "        action = torch.argmax(action_probs, dim=1).item()\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    #print(f\"Step: {info['step']}, Action: {info['action']}, Price: {info['price']}, Shares: {info['shares']}, Reward: {reward}\")\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "# Render the final state\n",
        "env.render()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZErdvofLUDi4",
        "outputId": "c7cb94b4-828c-46b1-a9c9-852932c5cf38"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 59235\n",
            "Balance: 9226381.18500003\n",
            "Shares held: 4007.0\n",
            "Total shares traded: 4007.0\n",
            "Total portfolio value: 9997528.33500003\n",
            "Cumulative reward: -12027.286438573909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tnZOj1jy4hNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Playground"
      ],
      "metadata": {
        "id": "RMg2KB4sUAoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class TradingEnvironment(gym.Env):\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, data, daily_trading_limit, transformer_model_path, seq_length=50):\n",
        "        super(TradingEnvironment, self).__init__()\n",
        "        self.data = data.reset_index(drop=True)  # Ensure the data index is reset\n",
        "        self.daily_trading_limit = daily_trading_limit\n",
        "        self.seq_length = seq_length\n",
        "        self.current_step = 0\n",
        "\n",
        "        # Extract state columns\n",
        "        self.state_columns = ['Close', 'Volume', 'RSI', 'MACD', 'MACD_signal', 'Stoch_k', 'Stoch_d',\n",
        "                              'OBV', 'Upper_BB', 'Middle_BB', 'Lower_BB', 'ATR_1', 'ADX', '+DI', '-DI', 'CCI']\n",
        "\n",
        "        # Fit the scaler on the training data\n",
        "        self.scaler = StandardScaler()\n",
        "        self.scaler.fit(self.data[self.state_columns])\n",
        "\n",
        "        # Load the transformer model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = TransformerModel(len(self.state_columns), 64, 4, 2, 3).to(self.device)\n",
        "        self.model.load_state_dict(torch.load(transformer_model_path, map_location=self.device))\n",
        "        self.model.eval()\n",
        "\n",
        "        # Initialize balance, shares held, and total shares traded\n",
        "        self.balance = 10_000_000.0  # $10 million\n",
        "        self.shares_held = 0\n",
        "        self.total_shares_traded = 0\n",
        "\n",
        "        # Define action space: [Hold, Buy, Sell]\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "\n",
        "        # Define observation space based on state columns\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf, high=np.inf, shape=(len(self.state_columns),), dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        self.balance = 10_000_000.0  # $10 million\n",
        "        self.shares_held = 0\n",
        "        self.total_shares_traded = 0\n",
        "        self.cumulative_reward = 0\n",
        "        self.trades = []\n",
        "        return self._next_observation()\n",
        "\n",
        "    def _next_observation(self):\n",
        "        return self.data[self.state_columns].iloc[self.current_step].values\n",
        "\n",
        "    def _get_scaled_observation(self):\n",
        "        obs = self.data[self.state_columns].iloc[self.current_step].values\n",
        "        scaled_obs = self.scaler.transform(obs.reshape(1, -1)).flatten()\n",
        "        return scaled_obs\n",
        "\n",
        "    def step(self, action):\n",
        "        expected_price = self.data.iloc[self.current_step]['ask_px_00']\n",
        "        actual_price = self.data.iloc[self.current_step]['price']\n",
        "        transaction_time = self.data.iloc[self.current_step]['ts_in_delta']\n",
        "        self._take_action(action)\n",
        "        reward = 0\n",
        "\n",
        "        if self.current_step >= len(self.data) - 1:\n",
        "            done = True\n",
        "        else:\n",
        "            done = False\n",
        "        if action != 0:\n",
        "            transaction_cost = self._calculate_transaction_cost(\n",
        "                self.data.iloc[self.current_step]['Volume'], 0.3, self.data['Volume'].mean()\n",
        "            )\n",
        "            reward = self._calculate_reward(expected_price, actual_price, transaction_time, transaction_cost)\n",
        "            self.cumulative_reward += reward\n",
        "            if self.trades:\n",
        "                self.trades[-1]['reward'] = reward\n",
        "                self.trades[-1]['transaction_cost'] = transaction_cost\n",
        "                self.trades[-1]['slippage'] = expected_price - actual_price\n",
        "                self.trades[-1]['time_penalty'] = 100 * transaction_time / 1e9\n",
        "\n",
        "        info = {\n",
        "            'step': self.current_step,\n",
        "            'action': action,\n",
        "            'price': actual_price,\n",
        "            'shares': self.trades[-1]['shares'] if self.trades else 0\n",
        "        }\n",
        "        self.current_step += 1\n",
        "\n",
        "        return self._next_observation(), reward, done, info\n",
        "\n",
        "    def _take_action(self, action):\n",
        "        current_price = self.data.iloc[self.current_step]['Close']\n",
        "        current_time = pd.to_datetime(self.data.iloc[self.current_step]['ts_event'])\n",
        "        trade_info = {'step': self.current_step, 'timestamp': current_time, 'action': action, 'price': current_price, 'shares': 0, 'reward': 0, 'transaction_cost': 0, 'slippage': 0, 'time_penalty': 0}\n",
        "\n",
        "        if action == 1:  # Buy\n",
        "            shares_bought = (self.balance * np.random.uniform(0.001, 0.005)) // current_price\n",
        "            self.balance -= shares_bought * current_price\n",
        "            self.shares_held += shares_bought\n",
        "            self.total_shares_traded += shares_bought\n",
        "            trade_info['shares'] = shares_bought\n",
        "            if shares_bought > 0:\n",
        "                self.trades.append(trade_info)\n",
        "        elif action == 2:  # Sell\n",
        "            shares_sold = min((self.balance * np.random.uniform(0.001, 0.005)) // current_price, self.shares_held)\n",
        "            self.balance += shares_sold * current_price\n",
        "            self.shares_held -= shares_sold\n",
        "            self.total_shares_traded -= shares_sold\n",
        "            trade_info['shares'] = shares_sold\n",
        "            if shares_sold > 0:\n",
        "                self.trades.append(trade_info)\n",
        "\n",
        "    def _calculate_reward(self, expected_price, actual_price, transaction_time, transaction_cost):\n",
        "        slippage = expected_price - actual_price\n",
        "        time_penalty = 100 * transaction_time / 1e9\n",
        "        reward = - (slippage + time_penalty + transaction_cost)\n",
        "        return reward\n",
        "\n",
        "    def _calculate_transaction_cost(self, volume, volatility, daily_volume):\n",
        "        return volatility * np.sqrt(volume / daily_volume)\n",
        "\n",
        "    def run(self):\n",
        "        self.reset()\n",
        "        for _ in range(len(self.data) - 1):\n",
        "            scaled_obs = self._get_scaled_observation()\n",
        "            with torch.no_grad():\n",
        "                obs_tensor = torch.tensor(scaled_obs, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(self.device)  # Add batch and sequence dimension\n",
        "                action_probs = self.model(obs_tensor)\n",
        "                action = torch.argmax(action_probs, dim=1).item()\n",
        "            obs, reward, done, info = self.step(action)\n",
        "            print(f\"Step: {info['step']}, Action: {info['action']}, Price: {info['price']}, Shares: {info['shares']}, Reward: {reward}\")\n",
        "            if done:\n",
        "                break\n",
        "        return self.cumulative_reward, self.trades\n",
        "\n",
        "    def render(self, mode='human', close=False):\n",
        "        print(f'Step: {self.current_step}')\n",
        "        print(f'Balance: {self.balance}')\n",
        "        print(f'Shares held: {self.shares_held}')\n",
        "        print(f'Total shares traded: {self.total_shares_traded}')\n",
        "        print(f'Total portfolio value: {self.balance + self.shares_held * self.data.iloc[self.current_step][\"Close\"]}')\n",
        "        print(f'Cumulative reward: {self.cumulative_reward}')\n",
        "        self.print_trades()\n",
        "\n",
        "    def print_trades(self):\n",
        "        trades_df = pd.DataFrame(self.trades)\n",
        "        trades_df.to_csv('trades_transformer.csv', index=False)\n",
        "        for trade in self.trades:\n",
        "            print(f\"Step: {trade['step']}, Timestamp: {trade['timestamp']}, Action: {trade['action']}, Price: {trade['price']}, Shares: {trade['shares']}, Reward: {trade['reward']}, Transaction Cost: {trade['transaction_cost']}, Slippage: {trade['slippage']}, Time Penalty: {trade['time_penalty']}\")\n"
      ],
      "metadata": {
        "id": "tYh0cuYeUCzY"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class TradingEnvironment(gym.Env):\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, data, daily_trading_limit, transformer_model_path, seq_length=50):\n",
        "        super(TradingEnvironment, self).__init__()\n",
        "        self.data = data.reset_index(drop=True)  # Ensure the data index is reset\n",
        "        self.daily_trading_limit = daily_trading_limit\n",
        "        self.seq_length = seq_length\n",
        "        self.current_step = 0\n",
        "\n",
        "        # Extract state columns\n",
        "        self.state_columns = ['Close', 'Volume', 'RSI', 'MACD', 'MACD_signal', 'Stoch_k', 'Stoch_d',\n",
        "                              'OBV', 'Upper_BB', 'Middle_BB', 'Lower_BB', 'ATR_1', 'ADX', '+DI', '-DI', 'CCI']\n",
        "\n",
        "        # Fit the scaler on the training data\n",
        "        self.scaler = StandardScaler()\n",
        "        self.scaler.fit(self.data[self.state_columns])\n",
        "\n",
        "        # Load the transformer model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = TransformerModel(len(self.state_columns), 64, 4, 2, 3).to(self.device)\n",
        "        self.model.load_state_dict(torch.load(transformer_model_path, map_location=self.device))\n",
        "        self.model.eval()\n",
        "\n",
        "        # Initialize balance, shares held, and total shares traded\n",
        "        self.balance = 10_000_000.0  # $10 million\n",
        "        self.shares_held = 0\n",
        "        self.total_shares_traded = 0\n",
        "\n",
        "        # Define action space: [Hold, Buy, Sell]\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "\n",
        "        # Define observation space based on state columns\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf, high=np.inf, shape=(len(self.state_columns),), dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        self.balance = 10_000_000.0  # $10 million\n",
        "        self.shares_held = 0\n",
        "        self.total_shares_traded = 0\n",
        "        self.cumulative_reward = 0\n",
        "        self.trades = []\n",
        "        return self._next_observation()\n",
        "\n",
        "    def _next_observation(self):\n",
        "        return self.data[self.state_columns].iloc[self.current_step].values\n",
        "\n",
        "    def _get_scaled_observation(self):\n",
        "        obs_df = self.data[self.state_columns].iloc[[self.current_step]]\n",
        "        scaled_obs = self.scaler.transform(obs_df).flatten()\n",
        "        return scaled_obs\n",
        "\n",
        "    def step(self, action):\n",
        "        expected_price = self.data.iloc[self.current_step]['ask_px_00']\n",
        "        actual_price = self.data.iloc[self.current_step]['price']\n",
        "        transaction_time = self.data.iloc[self.current_step]['ts_in_delta']\n",
        "        self._take_action(action)\n",
        "        reward = 0\n",
        "\n",
        "        if self.current_step >= len(self.data) - 1:\n",
        "            done = True\n",
        "        else:\n",
        "            done = False\n",
        "        if action != 0:\n",
        "            transaction_cost = self._calculate_transaction_cost(\n",
        "                self.data.iloc[self.current_step]['Volume'], 0.3, self.data['Volume'].mean()\n",
        "            )\n",
        "            reward = self._calculate_reward(expected_price, actual_price, transaction_time, transaction_cost)\n",
        "            self.cumulative_reward += reward\n",
        "            if self.trades:\n",
        "                self.trades[-1]['reward'] = reward\n",
        "                self.trades[-1]['transaction_cost'] = transaction_cost\n",
        "                self.trades[-1]['slippage'] = expected_price - actual_price\n",
        "                self.trades[-1]['time_penalty'] = 100 * transaction_time / 1e9\n",
        "\n",
        "        info = {\n",
        "            'step': self.current_step,\n",
        "            'action': action,\n",
        "            'price': actual_price,\n",
        "            'shares': self.trades[-1]['shares'] if self.trades else 0\n",
        "        }\n",
        "        self.current_step += 1\n",
        "\n",
        "        return self._next_observation(), reward, done, info\n",
        "\n",
        "    def _take_action(self, action):\n",
        "        current_price = self.data.iloc[self.current_step]['Close']\n",
        "        current_time = pd.to_datetime(self.data.iloc[self.current_step]['ts_event'])\n",
        "        trade_info = {'step': self.current_step, 'timestamp': current_time, 'action': action, 'price': current_price, 'shares': 0, 'reward': 0, 'transaction_cost': 0, 'slippage': 0, 'time_penalty': 0}\n",
        "\n",
        "        if action == 1:  # Buy\n",
        "            shares_bought = (self.balance * np.random.uniform(0.001, 0.005)) // current_price\n",
        "            self.balance -= shares_bought * current_price\n",
        "            self.shares_held += shares_bought\n",
        "            self.total_shares_traded += shares_bought\n",
        "            trade_info['shares'] = shares_bought\n",
        "            if shares_bought > 0:\n",
        "                self.trades.append(trade_info)\n",
        "        elif action == 2:  # Sell\n",
        "            shares_sold = min((self.balance * np.random.uniform(0.001, 0.005)) // current_price, self.shares_held)\n",
        "            self.balance += shares_sold * current_price\n",
        "            self.shares_held -= shares_sold\n",
        "            self.total_shares_traded -= shares_sold\n",
        "            trade_info['shares'] = shares_sold\n",
        "            if shares_sold > 0:\n",
        "                self.trades.append(trade_info)\n",
        "\n",
        "    def _calculate_reward(self, expected_price, actual_price, transaction_time, transaction_cost):\n",
        "        slippage = expected_price - actual_price\n",
        "        time_penalty = 100 * transaction_time / 1e9\n",
        "        reward = - (slippage + time_penalty + transaction_cost)\n",
        "        return reward\n",
        "\n",
        "    def _calculate_transaction_cost(self, volume, volatility, daily_volume):\n",
        "        return volatility * np.sqrt(volume / daily_volume)\n",
        "\n",
        "    def run(self):\n",
        "        self.reset()\n",
        "        for _ in range(len(self.data) - 1):\n",
        "            scaled_obs = self._get_scaled_observation()\n",
        "            with torch.no_grad():\n",
        "                obs_tensor = torch.tensor(scaled_obs, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(self.device)  # Add batch and sequence dimension\n",
        "                action_probs = self.model(obs_tensor)\n",
        "                action = torch.argmax(action_probs, dim=1).item()\n",
        "            obs, reward, done, info = self.step(action)\n",
        "            print(f\"Step: {info['step']}, Action: {info['action']}, Price: {info['price']}, Shares: {info['shares']}, Reward: {reward}\")\n",
        "            if done:\n",
        "                break\n",
        "        return self.cumulative_reward, self.trades\n",
        "\n",
        "    def render(self, mode='human', close=False):\n",
        "        print(f'Step: {self.current_step}')\n",
        "        print(f'Balance: {self.balance}')\n",
        "        print(f'Shares held: {self.shares_held}')\n",
        "        print(f'Total shares traded: {self.total_shares_traded}')\n",
        "        print(f'Total portfolio value: {self.balance + self.shares_held * self.data.iloc[self.current_step][\"Close\"]}')\n",
        "        print(f'Cumulative reward: {self.cumulative_reward}')\n",
        "        self.print_trades()\n",
        "\n",
        "    def print_trades(self):\n",
        "        trades_df = pd.DataFrame(self.trades)\n",
        "        trades_df.to_csv('trades_transformer.csv', index=False)\n",
        "        #for trade in self.trades:\n",
        "        #    print(f\"Step: {trade['step']}, Timestamp: {trade['timestamp']}, Action: {trade['action']}, Price: {trade['price']}, Shares: {trade['shares']}, Reward: {trade['reward']}, Transaction Cost: {trade['transaction_cost']}, Slippage: {trade['slippage']}, Time Penalty: {trade['time_penalty']}\")\n"
      ],
      "metadata": {
        "id": "XSXmNOGNVCtv"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wIUg-iG8UOIA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}